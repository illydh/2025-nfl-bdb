{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mROnIv9Bx3Nz"
      },
      "source": [
        "# Step: Downloading the dataset\n",
        "\n",
        "To begin, you must connect to runtime and have an api token from kaggle saved in your immediate directory of Google Drive in order to access the dataset directly through CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l2vl8dZIAeh",
        "outputId": "d62600b2-22f2-4f57-a5a9-80b3c11bc2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to mount Google Drive and get `kaggle.json` from personal directory\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq50wyvXbjg-",
        "outputId": "e22ef37d-6201-4741-c8e3-e35777b528df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Downloading nfl-big-data-bowl-2025.zip to /content\n",
            "100% 1.14G/1.14G [00:29<00:00, 38.5MB/s]\n",
            "100% 1.14G/1.14G [00:29<00:00, 40.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to download the competition dataset to notebook directory\n",
        "\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download nfl-big-data-bowl-2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AKonfF4J6O"
      },
      "source": [
        "### Import tracking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jVngtdStzraN"
      },
      "outputs": [],
      "source": [
        "file_location = '/content/nfl-big-data-bowl-2025.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xe1DWyLRxNil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "723802cc-05ed-4697-c3b4-5ed2c9ffb136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/nfl-big-data-bowl-2025.zip\n",
            "  inflating: games.csv               \n",
            "  inflating: player_play.csv         \n",
            "  inflating: players.csv             \n",
            "  inflating: plays.csv               \n",
            "  inflating: tracking_week_1.csv     \n",
            "  inflating: tracking_week_2.csv     \n",
            "  inflating: tracking_week_3.csv     \n",
            "  inflating: tracking_week_4.csv     \n",
            "  inflating: tracking_week_5.csv     \n",
            "  inflating: tracking_week_6.csv     \n",
            "  inflating: tracking_week_7.csv     \n",
            "  inflating: tracking_week_8.csv     \n",
            "  inflating: tracking_week_9.csv     \n"
          ]
        }
      ],
      "source": [
        "!unzip {file_location}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VSljM1e_35Y"
      },
      "source": [
        "# Stage: Prep Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX7AL9OW2r6c"
      },
      "source": [
        "## Load in the data files from the data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bzi2vcX-FSd5"
      },
      "outputs": [],
      "source": [
        "from argparse import ArgumentParser\n",
        "from pathlib import Path\n",
        "\n",
        "import polars as pl\n",
        "import os\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "# TEMPORARY CHANGE\n",
        "INPUT_DATA_DIR = Path(\"./\")\n",
        "SPLIT_OUT_DIR = Path(\"./drive/My Drive/bdb-2025/prepped_data/split\")\n",
        "SPLIT_OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "TRACKING_OUT_DIR = Path(\"./drive/My Drive/bdb-2025/prepped_data/tracking\")\n",
        "TRACKING_OUT_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e2PVgPUBB4m1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d7df61-006f-4380-94df-ae68f8c5e9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load players\n",
            "Load plays\n",
            "Load tracking\n",
            "tracking_df rows: 4445496\n",
            "Add features to tracking\n",
            "Remove null positions\n",
            "tracking_df rows: 4067734\n"
          ]
        }
      ],
      "source": [
        "def get_players_df() -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Load player-level data and preprocesses features.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Preprocessed player data with additional features.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        pl.read_csv(INPUT_DATA_DIR / \"players.csv\", null_values=[\"NA\", \"nan\", \"N/A\", \"NaN\", \"\"])\n",
        "        .with_columns(\n",
        "            height_inches=(\n",
        "                pl.col(\"height\").str.split(\"-\").map_elements(lambda s: int(s[0]) * 12 + int(s[1]), return_dtype=int)\n",
        "            )\n",
        "        )\n",
        "        .with_columns(\n",
        "            weight_Z=(pl.col(\"weight\") - pl.col(\"weight\").mean()) / pl.col(\"weight\").std(),\n",
        "            height_Z=(pl.col(\"height_inches\") - pl.col(\"height_inches\").mean()) / pl.col(\"height_inches\").std(),\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_plays_df() -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Load play-level data and preprocesses features.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Preprocessed play data with additional features.\n",
        "    \"\"\"\n",
        "    return pl.read_csv(INPUT_DATA_DIR / \"plays.csv\", null_values=[\"NA\", \"nan\", \"N/A\", \"NaN\", \"\"]).with_columns(\n",
        "        distanceToGoal=(\n",
        "            pl.when(pl.col(\"possessionTeam\") == pl.col(\"yardlineSide\"))\n",
        "            .then(100 - pl.col(\"yardlineNumber\"))\n",
        "            .otherwise(pl.col(\"yardlineNumber\"))\n",
        "        )\n",
        "    )\n",
        "\n",
        "def get_tracking_df() -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Load tracking data and preprocesses features. Notably, exclude rows representing the football's movement.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Preprocessed tracking data with additional features.\n",
        "    \"\"\"\n",
        "    # don't include football rows for this project.\n",
        "    # NOTE: Only processing week 1 for the sake of time.  Change \"1\" to \"*\" to process all weeks\n",
        "    return pl.read_csv(INPUT_DATA_DIR / \"tracking_week_1.csv\", null_values=[\"NA\", \"nan\", \"N/A\", \"NaN\", \"\"]).filter(\n",
        "        (pl.col(\"displayName\") != \"football\") & (pl.col(\"frameType\") == \"BEFORE_SNAP\")\n",
        "    )\n",
        "\n",
        "def add_features_to_tracking_df(\n",
        "    tracking_df: pl.DataFrame,\n",
        "    players_df: pl.DataFrame,\n",
        "    plays_df: pl.DataFrame,\n",
        ") -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Consolidates play and player level data into the tracking data.\n",
        "\n",
        "    Args:\n",
        "        tracking_df (pl.DataFrame): Tracking data\n",
        "        players_df (pl.DataFrame): Player data\n",
        "        plays_df (pl.DataFrame): Play data\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Tracking data with additional features.\n",
        "    \"\"\"\n",
        "    # add `is_ball_carrier`, `team_indicator`, and other features to tracking data\n",
        "    og_len = len(tracking_df)\n",
        "    tracking_df = (\n",
        "        tracking_df.join(\n",
        "            plays_df.select(\n",
        "                \"gameId\",\n",
        "                \"playId\",\n",
        "                \"defensiveTeam\"\n",
        "            ),\n",
        "            on=[\"gameId\", \"playId\"],\n",
        "            how=\"inner\",\n",
        "        )\n",
        "        .join(\n",
        "            players_df.select([\"nflId\", \"displayName\", \"position\"]).unique(), # select position column\n",
        "            on=[\"nflId\", \"displayName\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        #.join(\n",
        "        #    players_df.select([\"nflId\", \"weight_Z\", \"height_Z\"]).unique(),\n",
        "        #    on=\"nflId\",\n",
        "        #    how=\"inner\",\n",
        "        #)\n",
        "        .with_columns(\n",
        "            isDefense=pl.when(pl.col(\"club\") == pl.col(\"defensiveTeam\"))\n",
        "            .then(pl.lit(1))\n",
        "            .otherwise(pl.lit(-1))\n",
        "            .alias(\"isDefense\"),\n",
        "        )\n",
        "        .drop([\"defensiveTeam\"])\n",
        "    )\n",
        "\n",
        "    assert len(tracking_df) == og_len, \"Lost rows when joining tracking data with play/player data\"\n",
        "\n",
        "    return tracking_df\n",
        "\n",
        "def remove_null_positions(tracking_df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Remove rows with null positions.\n",
        "\n",
        "    Args:\n",
        "        tracking_df (pl.DataFrame): Tracking data\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Tracking data with null positions removed.\n",
        "    \"\"\"\n",
        "    # Players with null positions\n",
        "    null_position_players = tracking_df.filter(pl.col(\"position\").is_null())\n",
        "\n",
        "    # Identify unique gameId and playId combinations with null positions\n",
        "    null_plays = null_position_players.select([\"gameId\", \"playId\"]).unique()\n",
        "\n",
        "    # Filter out the plays with null positions from the tracking data\n",
        "    return tracking_df.join(\n",
        "        null_plays, on=[\"gameId\", \"playId\"], how=\"anti\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Load in raw data\n",
        "print(\"Load players\")\n",
        "players_df = get_players_df()\n",
        "print(\"Load plays\")\n",
        "plays_df = get_plays_df()\n",
        "print(\"Load tracking\")\n",
        "tracking_df = get_tracking_df()\n",
        "print(\"tracking_df rows:\", len(tracking_df))\n",
        "print(\"Add features to tracking\")\n",
        "tracking_df = add_features_to_tracking_df(tracking_df, players_df, plays_df)\n",
        "del players_df\n",
        "print(\"Remove null positions\")\n",
        "tracking_df = remove_null_positions(tracking_df)\n",
        "print(\"tracking_df rows:\", len(tracking_df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique player positions from players_df\n",
        "\n",
        "unique_positions = tracking_df['position'].unique().to_list()\n",
        "unique_positions = { pos: i for i, pos in enumerate(sorted(unique_positions)) }\n",
        "unique_positions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIY1bkFla9ZV",
        "outputId": "32542ee4-8e30-4cd0-b5c0-73846049522b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0,\n",
              " 'CB': 1,\n",
              " 'DE': 2,\n",
              " 'DT': 3,\n",
              " 'FB': 4,\n",
              " 'FS': 5,\n",
              " 'G': 6,\n",
              " 'ILB': 7,\n",
              " 'LB': 8,\n",
              " 'MLB': 9,\n",
              " 'NT': 10,\n",
              " 'OLB': 11,\n",
              " 'QB': 12,\n",
              " 'RB': 13,\n",
              " 'SS': 14,\n",
              " 'T': 15,\n",
              " 'TE': 16,\n",
              " 'WR': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83cSDxOGJfKW"
      },
      "source": [
        "NOTE: there two current `tracking` dataframes. One is the consolidated one, the other is augmented the direction of all players to assume that they are all moving in the same direction/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP4Co26H7HHe"
      },
      "source": [
        "## Explore model target --> Position of Masked player\n",
        "\n",
        "* Target dim.: (3, )\n",
        "  - first dim.: `displayName` of masked player\n",
        "  - second dim.: `x` coordinate of masked player on the field at the frame\n",
        "  - third dim.: `y` coordinate of masked player on the field at the frame\n",
        "\n",
        "## Goal: Randomly select a player and predict their x-y coordinates by removing them from the input sequence and having the model estimate their motion based on the locations of the 21 other players.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(tracking_df):\n",
        "    \"\"\"\n",
        "    Augments tracking data by iterating through each game and play,\n",
        "    removing one defensive player at a time and creating a new DataFrame.\n",
        "    Saves each play to a parquet file.\n",
        "\n",
        "    Args:\n",
        "        tracking_df (pl.DataFrame): Tracking data\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Augmented tracking data\n",
        "    \"\"\"\n",
        "    # Assuming 'tracking_df' is your Polars DataFrame\n",
        "    # and it has columns 'gameId', 'playId', 'x', 'y'\n",
        "    tgt_df = []\n",
        "    for game_id, play_id in tracking_df.select([\"gameId\", \"playId\"]).unique().rows():\n",
        "        defensive_players = tracking_df.filter((pl.col(\"gameId\") == game_id) & (pl.col(\"playId\") == play_id) & (pl.col(\"isDefense\") == 1))\n",
        "        unique_names = defensive_players['displayName'].unique().to_list()\n",
        "\n",
        "        rel_tracking_df = []\n",
        "        player_identifier = 1 # Initialize player identifier\n",
        "\n",
        "        for player_name in unique_names:\n",
        "            # Filter out the selected player from tracking_df\n",
        "            # filtered_df = tracking_df.filter(\n",
        "            #         (pl.col(\"gameId\") == game_id)\n",
        "            #         & (pl.col(\"playId\") == play_id)\n",
        "            #         & (pl.col(\"displayName\") != player_name)\n",
        "            # )\n",
        "            filtered_df = tracking_df.filter(\n",
        "                (pl.col(\"gameId\") == game_id)\n",
        "                & (pl.col(\"playId\") == play_id)\n",
        "                & (pl.col(\"displayName\") != player_name)\n",
        "            ).with_columns(\n",
        "                pl.lit(player_identifier).alias(\"maskedId\") # Add player identifier\n",
        "            )\n",
        "\n",
        "            masked_player_df = tracking_df.filter(\n",
        "                (pl.col(\"gameId\") == game_id)\n",
        "                & (pl.col(\"playId\") == play_id)\n",
        "                & (pl.col(\"displayName\") == player_name)\n",
        "            ).with_columns(\n",
        "                pl.lit(player_identifier).alias(\"maskedId\") # Add player identifier\n",
        "            )\n",
        "\n",
        "            # Assert that player_name is not in the filtered dataframe\n",
        "            assert player_name not in filtered_df[\"displayName\"].to_list(), f\"Player {player_name} still present in filtered data for game {game_id}, play {play_id}\"\n",
        "            assert (player_name in masked_player_df[\"displayName\"].to_list()), f\"Player {player_name} not present in masked data for game {game_id}, play {play_id}\"\n",
        "\n",
        "            rel_tracking_df.append(filtered_df)\n",
        "            tgt_df.append(masked_player_df)\n",
        "            player_identifier += 1\n",
        "\n",
        "        # Concatenate all DataFrames in the list\n",
        "        rel_tracking_df = pl.concat(rel_tracking_df)\n",
        "\n",
        "        # Select only the specified columns\n",
        "        rel_tracking_df = rel_tracking_df.select(\n",
        "            [\"gameId\", \"playId\", \"frameId\", \"maskedId\", \"nflId\", \"displayName\", \"position\", \"x\", \"y\"]\n",
        "        )\n",
        "\n",
        "        # Save the DataFrame to the specified directory\n",
        "        rel_tracking_df.write_parquet(TRACKING_OUT_DIR / f\"game_{game_id}_play_{play_id}.parquet\")\n",
        "\n",
        "    return pl.concat(tgt_df).select(\n",
        "        [\"gameId\", \"playId\", \"frameId\", \"maskedId\", \"displayName\", \"position\", \"x\", \"y\"]\n",
        "        )\n",
        "\n",
        "print(f\"Augment tracking data\")\n",
        "tgt_df = augment_data(tracking_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztYLfyolqBX8",
        "outputId": "8c68d99c-3932-4178-a236-397ee60a0ed3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augment tracking data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuQWR_B8dHFZ"
      },
      "source": [
        "## Splitting data into train, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6PArmNwkdGVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd14c1cb-3279-4976-8b4f-0b0d5da4ea48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split train/test/val\n",
            "Train set: 1258 plays, 129974 frames\n",
            "Test set: 269 plays, 27994 frames\n",
            "Validation set: 269 plays, 26929 frames\n",
            "Total set: 1796 plays, 184897 frames\n"
          ]
        }
      ],
      "source": [
        "def split_train_test_val(target_df: pl.DataFrame) -> dict[str, pl.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split data into train, validation, and test sets.\n",
        "    Split is 70-15-15 for train-test-val respectively. Notably, we split at the play levle and not frame level.\n",
        "    This ensures no target contamination between splits.\n",
        "\n",
        "    Args:\n",
        "        tracking_df (pl.DataFrame): Tracking data\n",
        "        target_df (pl.DataFrame): Target data\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing train, validation, and test dataframes.\n",
        "    \"\"\"\n",
        "    target_df = target_df.sort([\"gameId\", \"playId\"])\n",
        "\n",
        "    test_val_ids = target_df.select([\"gameId\", \"playId\"]).unique(maintain_order=True).sample(fraction=0.3, seed=42)\n",
        "    train_tgt_df = target_df.join(test_val_ids, on=[\"gameId\", \"playId\"], how=\"anti\")\n",
        "    train_ids = train_tgt_df.select([\"gameId\", \"playId\"]).unique(maintain_order=True)\n",
        "    train_tracking_df = [ pl.read_parquet(TRACKING_OUT_DIR / f\"game_{game_id}_play_{play_id}.parquet\") for game_id, play_id in train_ids.rows() ]\n",
        "    train_tracking_df = pl.concat(train_tracking_df)\n",
        "    print(\n",
        "        f\"Train set: {train_tracking_df.n_unique(['gameId', 'playId'])} plays,\",\n",
        "        f\"{train_tracking_df.n_unique(['gameId', 'playId', 'frameId'])} frames\",\n",
        "    )\n",
        "\n",
        "    test_ids = test_val_ids.sample(fraction=0.5, seed=42)  # 70-15-15 split\n",
        "    test_tgt_df = target_df.join(test_ids, on=[\"gameId\", \"playId\"], how=\"inner\")\n",
        "    test_tracking_df = [ pl.read_parquet(TRACKING_OUT_DIR / f\"game_{game_id}_play_{play_id}.parquet\") for game_id, play_id in test_ids.rows() ]\n",
        "    test_tracking_df = pl.concat(test_tracking_df)\n",
        "    print(\n",
        "        f\"Test set: {test_tracking_df.n_unique(['gameId', 'playId'])} plays,\",\n",
        "        f\"{test_tracking_df.n_unique(['gameId', 'playId', 'frameId'])} frames\",\n",
        "    )\n",
        "\n",
        "    val_ids = test_val_ids.join(test_ids, on=[\"gameId\", \"playId\"], how=\"anti\")\n",
        "    val_tgt_df = target_df.join(val_ids, on=[\"gameId\", \"playId\"], how=\"inner\")\n",
        "    val_tracking_df = [ pl.read_parquet(TRACKING_OUT_DIR / f\"game_{game_id}_play_{play_id}.parquet\") for game_id, play_id in val_ids.rows() ]\n",
        "    val_tracking_df = pl.concat(val_tracking_df)\n",
        "    print(\n",
        "        f\"Validation set: {val_tracking_df.n_unique(['gameId', 'playId'])} plays,\",\n",
        "        f\"{val_tracking_df.n_unique(['gameId', 'playId','frameId'])} frames\",\n",
        "    )\n",
        "\n",
        "    len_plays_tracking_df = train_tracking_df.n_unique(['gameId', 'playId']) + test_tracking_df.n_unique(['gameId', 'playId']) + val_tracking_df.n_unique(['gameId', 'playId'])\n",
        "    len_frames_tracking_df = train_tracking_df.n_unique(['gameId', 'playId','frameId']) + test_tracking_df.n_unique(['gameId', 'playId','frameId']) + val_tracking_df.n_unique(['gameId', 'playId','frameId'])\n",
        "    print(\n",
        "        f\"Total set: {len_plays_tracking_df} plays,\",\n",
        "        f\"{len_frames_tracking_df} frames\",\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"train_features\": train_tracking_df,\n",
        "        \"train_targets\": train_tgt_df,\n",
        "        \"test_features\": test_tracking_df,\n",
        "        \"test_targets\": test_tgt_df,\n",
        "        \"val_features\": val_tracking_df,\n",
        "        \"val_targets\": val_tgt_df,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Split train/test/val\")\n",
        "split_dfs = split_train_test_val(tgt_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eQ42Knvq9SKn"
      },
      "outputs": [],
      "source": [
        "# Export splits to OUT_DIR\n",
        "\n",
        "for key, df in split_dfs.items():\n",
        "    sort_keys = [\"gameId\", \"playId\", \"frameId\"]\n",
        "    df.sort(sort_keys).write_parquet(SPLIT_OUT_DIR / f\"{key}.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vnRFlGTgoJa8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}