{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV5tVwrd0WHZ",
        "outputId": "49a34869-ec12-43b6-db7e-3c822f42db47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to mount Google Drive and get `kaggle.json` from personal directory\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing Dependencies\n",
        "\n",
        "import multiprocessing as mp\n",
        "import pickle\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xMgcyb2207Yd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPORARY CHANGE\n",
        "PREPPED_DATA_DIR = Path(\"./drive/My Drive/bdb-2025/split_prepped_data/\")\n",
        "DATASET_DIR = Path(\"./drive/My Drive/bdb-2025/working/datasets/\")"
      ],
      "metadata": {
        "id": "1goYQ9Vf0-rE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enumerate player positions\n",
        "POSITIONS_ENUM = {\n",
        "    'T': 0,\n",
        "    'CB': 1,\n",
        "    'G': 2,\n",
        "    'SS': 3,\n",
        "    'FS': 4,\n",
        "    'DB': 5,\n",
        "    'MLB': 6,\n",
        "    'LB': 7,\n",
        "    'OLB': 8,\n",
        "    'DT': 9,\n",
        "    'ILB': 10,\n",
        "    'NT': 11,\n",
        "    'TE': 12,\n",
        "    'WR': 13,\n",
        "    'QB': 14,\n",
        "    'RB': 15,\n",
        "    'DE': 16,\n",
        "    'FB': 17,\n",
        "    'C': 18\n",
        "}"
      ],
      "metadata": {
        "id": "4GV0aj0USLrT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step: Processing Data\n",
        "\n"
      ],
      "metadata": {
        "id": "XLkUiGhG0tba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BDB2025_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for NFL tracking data.\n",
        "\n",
        "    This class preprocesses and stores NFL tracking data for use in machine learning models.\n",
        "\n",
        "    Attributes:\n",
        "        model_type (str): Type of model ('transformer')\n",
        "        keys (list): List of unique identifiers for each data point\n",
        "        feature_df_partition (pd.DataFrame): Preprocessed feature data\n",
        "        tgt_df_partition (pd.DataFrame): Preprocessed target data\n",
        "        tgt_arrays (dict): Precomputed target arrays\n",
        "        feature_arrays (dict): Precomputed feature arrays\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str,\n",
        "        feature_df: pl.DataFrame,\n",
        "        tgt_df: pl.DataFrame,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): Type of model ('transformer')\n",
        "            feature_df (pl.DataFrame): DataFrame containing feature data\n",
        "            tgt_df (pl.DataFrame): DataFrame containing target data\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an invalid model_type is provided\n",
        "        \"\"\"\n",
        "        if model_type not in [\"transformer\"]:\n",
        "            raise ValueError(\"model_type must be 'transformer'\")\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.keys = list(feature_df.select([\"gameId\", \"playId\", \"frameId\"]).unique().rows())\n",
        "\n",
        "        # Convert to pandas form with index for quick row retrieval\n",
        "        self.feature_df_partition = (\n",
        "            feature_df.to_pandas(use_pyarrow_extension_array=True)\n",
        "            .set_index([\"gameId\", \"playId\", \"frameId\", \"nflId\"])\n",
        "            .sort_index()\n",
        "        )\n",
        "        self.tgt_df_partition = (\n",
        "            tgt_df.to_pandas(use_pyarrow_extension_array=True)\n",
        "            .set_index([\"gameId\", \"playId\", \"frameId\"])\n",
        "            .sort_index()\n",
        "        )\n",
        "\n",
        "        # Precompute features and store in dicts\n",
        "        self.tgt_arrays: dict[tuple, np.ndarray] = {}\n",
        "        self.feature_arrays: dict[tuple, np.ndarray] = {}\n",
        "        with mp.Pool(processes=min(8, mp.cpu_count())) as pool:\n",
        "            results = pool.map(\n",
        "                self.process_key,\n",
        "                tqdm(self.keys, desc=\"Pre-computing feature transforms\", total=len(self.keys)),\n",
        "            )\n",
        "            # Unpack results\n",
        "            for key, tgt_array, feature_array in results:\n",
        "                self.tgt_arrays[key] = tgt_array\n",
        "                self.feature_arrays[key] = feature_array\n",
        "\n",
        "    def process_key(self, key: tuple) -> tuple[tuple, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Process a single key to generate target and feature arrays.\n",
        "\n",
        "        Args:\n",
        "            key (tuple): Key (gameId, playId, mirrored, frameId) identifying a specific data point\n",
        "\n",
        "        Returns:\n",
        "            tuple[tuple, np.ndarray, np.ndarray]: Processed key, target array, and feature array\n",
        "        \"\"\"\n",
        "        tgt_array = self.transform_target_df(self.tgt_df_partition.loc[key])\n",
        "        feature_array = self.transform_input_frame_df(self.feature_df_partition.loc[key])\n",
        "        return key, tgt_array, feature_array\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of samples in the dataset\n",
        "        \"\"\"\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            tuple[np.ndarray, np.ndarray]: Feature array and target array for the specified index\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the index is out of range\n",
        "        \"\"\"\n",
        "        if idx < 0 or idx >= len(self):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "        key = self.keys[idx]\n",
        "        return self.feature_arrays[key], self.tgt_arrays[key]\n",
        "\n",
        "    def transform_input_frame_df(self, frame_df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Transform input frame DataFrame to numpy array based on model type.\n",
        "\n",
        "        Args:\n",
        "            frame_df (pd.DataFrame): Input frame DataFrame\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Transformed input features\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an unknown model type is specified\n",
        "        \"\"\"\n",
        "\n",
        "        return self.transformer_transform_input_frame_df(frame_df)\n",
        "\n",
        "    def transform_target_df(self, tgt_df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Transform target DataFrame to numpy array.\n",
        "        Args:\n",
        "            tgt_df (pd.DataFrame): Target DataFrame\n",
        "        Returns:\n",
        "            np.ndarray: Transformed target values as one-hot encoded array\n",
        "        \"\"\"\n",
        "        # Create one-hot encoding with prefix\n",
        "        ohe_tgt = pd.get_dummies(tgt_df['position'], prefix='position')\n",
        "\n",
        "        # Ensure all formation types are present\n",
        "        expected_columns = [f\"position_{position}\" for position in sorted(POSITIONS_ENUM.keys())]\n",
        "        for col in expected_columns:\n",
        "            if col not in ohe_tgt.columns:\n",
        "                ohe_tgt[col] = 0\n",
        "\n",
        "        # Sort columns\n",
        "        ohe_tgt = ohe_tgt[expected_columns].to_numpy()[0].astype(np.float32)\n",
        "        x_tgt, y_tgt = np.array(tgt_df[\"x\"]).flatten(), np.array(tgt_df[\"y\"]).flatten()\n",
        "\n",
        "        # Reshape to ensure each feature has the same first dimension\n",
        "        # ohe_tgt = ohe_tgt.reshape(1, -1)\n",
        "        # x_tgt = x_tgt.reshape(1, -1)\n",
        "        # y_tgt = y_tgt.reshape(1, -1)\n",
        "\n",
        "        tgts = [ohe_tgt, x_tgt, y_tgt]\n",
        "        y = np.concatenate(tgts, dtype=np.float32)\n",
        "        assert y.shape == (21,), f\"Expected y.shape (21,), got {y.shape}\"\n",
        "        return y\n",
        "\n",
        "    def transformer_transform_input_frame_df(self, frame_df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Transform input frame DataFrame for transformer model.\n",
        "\n",
        "        Args:\n",
        "            frame_df (pd.DataFrame): Input frame DataFrame\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Transformed input features for transformer model\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: If the output shape is not as expected\n",
        "        \"\"\"\n",
        "        # Create one-hot encoding with prefix\n",
        "        ohe_feature = pd.get_dummies(frame_df['position'], prefix='position')\n",
        "        # Ensure all formation types are present\n",
        "        expected_columns = [f\"position_{position}\" for position in sorted(POSITIONS_ENUM.keys())]\n",
        "        for col in expected_columns:\n",
        "            if col not in ohe_feature.columns:\n",
        "                ohe_feature[col] = 0\n",
        "\n",
        "        # Sort columns\n",
        "        ohe_feature = ohe_feature[expected_columns].to_numpy().astype(np.float32)\n",
        "        x_feature, y_feature = np.array(frame_df[\"x\"]).flatten(), np.array(frame_df[\"y\"]).flatten()\n",
        "\n",
        "        assert len(frame_df) == 21, \"NOT THE SAME LEN\"\n",
        "\n",
        "        # Reshape features to have the same first dimension\n",
        "        ohe_feature = ohe_feature.reshape(len(frame_df), -1)\n",
        "        x_feature = x_feature.reshape(len(frame_df), 1)\n",
        "        y_feature = y_feature.reshape(len(frame_df), 1)\n",
        "\n",
        "        features = [ohe_feature, x_feature, y_feature]\n",
        "        x = np.concatenate(features, dtype=np.float32, axis=1)\n",
        "\n",
        "        #assert x.shape == (21, len(features)), f\"Expected x.shape (21, {len(features)}), got {x.shape}\"\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_datasets(model_type: str, split: str) -> BDB2025_Dataset:\n",
        "    \"\"\"\n",
        "    Load datasets for a specific model type and data split.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): Type of model ('transformer' or 'zoo')\n",
        "        split (str): Data split ('train', 'val', or 'test')\n",
        "\n",
        "    Returns:\n",
        "        BDB2025_Dataset: Loaded dataset for the specified model type and split\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unknown split is specified\n",
        "        FileNotFoundError: If the dataset file is not found\n",
        "    \"\"\"\n",
        "    ds_dir = DATASET_DIR / model_type\n",
        "    file_path = ds_dir / f\"{split}_dataset.pkl\"\n",
        "\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "a0pT893uCrTc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to create and save datasets for different model types and splits.\n",
        "    \"\"\"\n",
        "    for split in [\"test\", \"val\", \"train\"]:\n",
        "        feature_df = pl.read_parquet(PREPPED_DATA_DIR / f\"{split}_features.parquet\")\n",
        "        tgt_df = pl.read_parquet(PREPPED_DATA_DIR / f\"{split}_targets.parquet\")\n",
        "        for model_type in [\"transformer\"]:\n",
        "            print(f\"Creating dataset for {model_type=}, {split=}...\")\n",
        "            tic = time.time()\n",
        "            dataset = BDB2025_Dataset(model_type, feature_df, tgt_df)\n",
        "            out_dir = DATASET_DIR / model_type\n",
        "            out_dir.mkdir(exist_ok=True, parents=True)\n",
        "            with open(out_dir / f\"{split}_dataset.pkl\", \"wb\") as f:\n",
        "                pickle.dump(dataset, f)\n",
        "            print(f\"Took {(time.time() - tic)/60:.1f} mins\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7aC2StdV_XQ",
        "outputId": "36a6c3ed-6a0c-4387-9133-71062714b3d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset for model_type='transformer', split='test'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pre-computing feature transforms: 100%|██████████| 42793/42793 [05:58<00:00, 119.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 8.1 mins\n",
            "Creating dataset for model_type='transformer', split='val'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pre-computing feature transforms: 100%|██████████| 41947/41947 [05:55<00:00, 117.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 8.1 mins\n",
            "Creating dataset for model_type='transformer', split='train'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pre-computing feature transforms: 100%|██████████| 198599/198599 [28:22<00:00, 116.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 38.7 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j-QsVZ819ud7"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}