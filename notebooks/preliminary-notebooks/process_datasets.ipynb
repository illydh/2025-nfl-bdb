{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV5tVwrd0WHZ",
        "outputId": "d4563b15-7ef3-4e45-f627-0877920e3059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to mount Google Drive and get `kaggle.json` from personal directory\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing Dependencies\n",
        "\n",
        "import multiprocessing as mp\n",
        "import pickle\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xMgcyb2207Yd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPORARY CHANGE\n",
        "PREPPED_DATA_DIR = Path(\"./drive/My Drive/bdb-2025/split_prepped_data/\")\n",
        "DATASET_DIR = Path(\"./drive/My Drive/bdb-2025/working/datasets/\")\n",
        "\n",
        "DATASET_DIR.mkdir(parents=True, exist_ok=True) # Create directory if it doesn't exist"
      ],
      "metadata": {
        "id": "1goYQ9Vf0-rE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step: Processing Data\n",
        "\n"
      ],
      "metadata": {
        "id": "XLkUiGhG0tba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BDB2025_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for NFL tracking data.\n",
        "\n",
        "    This class preprocesses and stores NFL tracking data for use in machine learning models.\n",
        "\n",
        "    Attributes:\n",
        "        model_type (str): Type of model ('transformer')\n",
        "        keys (list): List of unique identifiers for each data point\n",
        "        feature_df_partition (pd.DataFrame): Preprocessed feature data\n",
        "        tgt_df_partition (pd.DataFrame): Preprocessed target data\n",
        "        tgt_arrays (dict): Precomputed target arrays\n",
        "        feature_arrays (dict): Precomputed feature arrays\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_type: str,\n",
        "        feature_df: pl.DataFrame,\n",
        "        tgt_df: pl.DataFrame,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): Type of model ('transformer')\n",
        "            feature_df (pl.DataFrame): DataFrame containing feature data\n",
        "            tgt_df (pl.DataFrame): DataFrame containing target data\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an invalid model_type is provided\n",
        "        \"\"\"\n",
        "        if model_type not in [\"transformer\"]:\n",
        "            raise ValueError(\"model_type must be 'transformer'\")\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.keys = list(feature_df.select([\"gameId\", \"playId\", \"frameId\"]).unique().rows())\n",
        "\n",
        "        # Convert to pandas form with index for quick row retrieval\n",
        "        self.feature_df_partition = (\n",
        "            feature_df.to_pandas(use_pyarrow_extension_array=True)\n",
        "            .set_index([\"gameId\", \"playId\", \"frameId\", \"nflId\"])\n",
        "            .sort_index()\n",
        "        )\n",
        "        self.tgt_df_partition = (\n",
        "            tgt_df.to_pandas(use_pyarrow_extension_array=True)\n",
        "            .set_index([\"gameId\", \"playId\", \"frameId\"])\n",
        "            .sort_index()\n",
        "        )\n",
        "\n",
        "        # Precompute features and store in dicts\n",
        "        self.tgt_arrays: dict[tuple, np.ndarray] = {}\n",
        "        self.feature_arrays: dict[tuple, np.ndarray] = {}\n",
        "        with mp.Pool(processes=min(8, mp.cpu_count())) as pool:\n",
        "            results = pool.map(\n",
        "                self.process_key,\n",
        "                tqdm(self.keys, desc=\"Pre-computing feature transforms\", total=len(self.keys)),\n",
        "            )\n",
        "            # Unpack results\n",
        "            for key, tgt_array, feature_array in results:\n",
        "                self.tgt_arrays[key] = tgt_array\n",
        "                self.feature_arrays[key] = feature_array\n",
        "\n",
        "    def process_key(self, key: tuple) -> tuple[tuple, np.ndarray, np.ndarray]:    #   CHECKED\n",
        "        \"\"\"\n",
        "        Process a single key to generate target and feature arrays.\n",
        "\n",
        "        Args:\n",
        "            key (tuple): Key (gameId, playId, mirrored, frameId) identifying a specific data point\n",
        "\n",
        "        Returns:\n",
        "            tuple[tuple, np.ndarray, np.ndarray]: Processed key, target array, and feature array\n",
        "        \"\"\"\n",
        "        tgt_array = self.transform_target_df(self.tgt_df_partition.loc[key])\n",
        "        feature_array = self.transform_input_frame_df(self.feature_df_partition.loc[key])\n",
        "        return key, tgt_array, feature_array\n",
        "\n",
        "    def __len__(self) -> int:     #   CHECKED\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of samples in the dataset\n",
        "        \"\"\"\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[np.ndarray, np.ndarray]:   #   CHECKED\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the item to retrieve\n",
        "\n",
        "        Returns:\n",
        "            tuple[np.ndarray, np.ndarray]: Feature array and target array for the specified index\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the index is out of range\n",
        "        \"\"\"\n",
        "        if idx < 0 or idx >= len(self):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "        key = self.keys[idx]\n",
        "        return self.feature_arrays[key], self.tgt_arrays[key]\n",
        "\n",
        "    def transform_input_frame_df(self, frame_df: pd.DataFrame) -> np.ndarray:   # CHECKED\n",
        "        \"\"\"\n",
        "        Transform input frame DataFrame to numpy array based on model type.\n",
        "\n",
        "        Args:\n",
        "            frame_df (pd.DataFrame): Input frame DataFrame\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Transformed input features\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If an unknown model type is specified\n",
        "        \"\"\"\n",
        "\n",
        "        return self.transformer_transform_input_frame_df(frame_df)\n",
        "\n",
        "    def transform_target_df(self, tgt_df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Transforms the target dataframe into a NumPy array containing the missing player's information.\n",
        "\n",
        "        Args:\n",
        "            tgt_df (pd.DataFrame): Target dataframe with missing player data.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array with each row containing the missing player's display name (as index)\n",
        "                        and their x-y coordinates.\n",
        "        \"\"\"\n",
        "        # Check if tgt_df is a Series and convert to DataFrame if necessary\n",
        "        if isinstance(tgt_df, pd.Series):\n",
        "            tgt_df = tgt_df.to_frame().T  # Convert Series to DataFrame\n",
        "\n",
        "        # Check required columns\n",
        "        required_columns = [\"displayName\", \"x\", \"y\"]\n",
        "        missing_columns = [col for col in required_columns if col not in tgt_df.columns]\n",
        "        if missing_columns:\n",
        "            # Instead of raising an error, print a warning and return an empty array\n",
        "            print(f\"Warning: Target dataframe is missing columns: {missing_columns} for key {tgt_df.index[0]}\")\n",
        "            raise ValueError(f\"Target dataframe must contain columns: {required_columns}\")\n",
        "\n",
        "\n",
        "        # Select relevant columns\n",
        "        transformed_df = tgt_df[[\"displayName\", \"x\", \"y\"]]\n",
        "\n",
        "        # Convert to NumPy array\n",
        "        y = transformed_df.to_numpy()\n",
        "\n",
        "        return y\n",
        "\n",
        "    def transformer_transform_input_frame_df(self, frame_df: pd.DataFrame) -> np.ndarray:   #   CHECKED AND EDITED\n",
        "        \"\"\"\n",
        "        Transform input frame DataFrame for transformer model.\n",
        "\n",
        "        Args:\n",
        "            frame_df (pd.DataFrame): Input frame DataFrame\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Transformed input features for transformer model\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: If the output shape is not as expected\n",
        "        \"\"\"\n",
        "        features = [\"x\", \"y\"]\n",
        "        x = frame_df[features].to_numpy(dtype=np.float32)\n",
        "        assert x.shape == (21, len(features)), f\"Expected shape (21, {len(features)}), got {x.shape}\"\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_datasets(model_type: str, split: str) -> BDB2025_Dataset:\n",
        "    \"\"\"\n",
        "    Load datasets for a specific model type and data split.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): Type of model ('transformer' or 'zoo')\n",
        "        split (str): Data split ('train', 'val', or 'test')\n",
        "\n",
        "    Returns:\n",
        "        BDB2025_Dataset: Loaded dataset for the specified model type and split\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unknown split is specified\n",
        "        FileNotFoundError: If the dataset file is not found\n",
        "    \"\"\"\n",
        "    ds_dir = DATASET_DIR / model_type\n",
        "    file_path = ds_dir / f\"{split}_dataset.pkl\"\n",
        "\n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "a0pT893uCrTc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to create and save datasets for different model types and splits.\n",
        "    \"\"\"\n",
        "    for split in [\"test\", \"val\", \"train\"]:\n",
        "        feature_df = pl.read_parquet(PREPPED_DATA_DIR / f\"{split}_features.parquet\")\n",
        "        tgt_df = pl.read_parquet(PREPPED_DATA_DIR / f\"{split}_targets.parquet\")\n",
        "        for model_type in [\"transformer\"]:\n",
        "            print(f\"Creating dataset for {model_type=}, {split=}...\")\n",
        "            tic = time.time()\n",
        "            dataset = BDB2025_Dataset(model_type, feature_df, tgt_df)\n",
        "            out_dir = DATASET_DIR / model_type\n",
        "            out_dir.mkdir(exist_ok=True, parents=True)\n",
        "            with open(out_dir / f\"{split}_dataset.pkl\", \"wb\") as f:\n",
        "                pickle.dump(dataset, f)\n",
        "            print(f\"Took {(time.time() - tic)/60:.1f} mins\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7aC2StdV_XQ",
        "outputId": "6ae5aefc-b57f-430f-a95b-ed501181654c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset for model_type='transformer', split='test'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pre-computing feature transforms: 100%|██████████| 46712/46712 [02:00<00:00, 387.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 2.7 mins\n",
            "Creating dataset for model_type='transformer', split='val'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pre-computing feature transforms: 100%|██████████| 47395/47395 [02:02<00:00, 387.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 2.8 mins\n",
            "Creating dataset for model_type='transformer', split='train'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pre-computing feature transforms: 100%|██████████| 214793/214793 [09:28<00:00, 377.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 13.0 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-QsVZ819ud7"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}